{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcG8GTv3N2EftU8dUz5Dzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "295ab24c993e4d6ba4b955b77f621db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dc23c73a6024bafb11a614a2b3d6873",
              "IPY_MODEL_810c3e1deada411494bb59d17798029d",
              "IPY_MODEL_9e7ec47e3314422a800ecef2d496f592"
            ],
            "layout": "IPY_MODEL_8964012439df4029a9a305d1459242e0"
          }
        },
        "0dc23c73a6024bafb11a614a2b3d6873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe60aa9fa71e48109153f1ffd0f041db",
            "placeholder": "​",
            "style": "IPY_MODEL_f26e15b34e7544ce88ac6323cf83e3cf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "810c3e1deada411494bb59d17798029d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2160208cb70413aa17034d1e0df9d63",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98d36b7c6e194c56b32c93428f243a7e",
            "value": 2
          }
        },
        "9e7ec47e3314422a800ecef2d496f592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ca3e695c5dc44859347d9e685616afd",
            "placeholder": "​",
            "style": "IPY_MODEL_ca9d9899d540478cbe55f9dd5ad856ef",
            "value": " 2/2 [00:35&lt;00:00, 16.87s/it]"
          }
        },
        "8964012439df4029a9a305d1459242e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe60aa9fa71e48109153f1ffd0f041db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26e15b34e7544ce88ac6323cf83e3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2160208cb70413aa17034d1e0df9d63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d36b7c6e194c56b32c93428f243a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ca3e695c5dc44859347d9e685616afd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca9d9899d540478cbe55f9dd5ad856ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zohreh6384NKH/LLM_Project/blob/main/chapter3_looking_inside_Transformer_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "295ab24c993e4d6ba4b955b77f621db5",
            "0dc23c73a6024bafb11a614a2b3d6873",
            "810c3e1deada411494bb59d17798029d",
            "9e7ec47e3314422a800ecef2d496f592",
            "8964012439df4029a9a305d1459242e0",
            "fe60aa9fa71e48109153f1ffd0f041db",
            "f26e15b34e7544ce88ac6323cf83e3cf",
            "a2160208cb70413aa17034d1e0df9d63",
            "98d36b7c6e194c56b32c93428f243a7e",
            "1ca3e695c5dc44859347d9e685616afd",
            "ca9d9899d540478cbe55f9dd5ad856ef"
          ]
        },
        "id": "mfl-RK_uncDg",
        "outputId": "c13fc75c-e567-4ae8-ff7d-d4b711c78fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "295ab24c993e4d6ba4b955b77f621db5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",                    # model\n",
        "    trust_remote_code=True,                # custom code for this code to be used\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input and Output of a trained transformer LLM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1kBDdQjS84A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "output = generator(prompt)\n",
        "print(output)   #it returns a list of outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxFEArxloJlP",
        "outputId": "3a42fbd4-ff23-4fe0-a4f9-476a720084e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \" Mention the steps you're taking to prevent it in the future.\\n\\nDear Sarah,\\n\\nI hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0])    # access to first element in the list which is a dictionary containing keys."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H8_MFkx-xBU",
        "outputId": "c72b7f1d-2068-4a85-f0f1-9f515b0f5320"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'generated_text': \" Mention the steps you're taking to prevent it in the future.\\n\\nDear Sarah,\\n\\nI hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lhuig9r_P5I",
        "outputId": "825463ed-90e3-4cd3-b6a8-56c6f4f5634d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oIao1QyIXAU",
        "outputId": "9ce827a0-bfe8-441a-8047-d871e180b02d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "          (rotary_emb): Phi3RotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm()\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_attention_layernorm): Phi3RMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a single token from the probability distribution (sampling / decoding)"
      ],
      "metadata": {
        "id": "ZaO8k4eQLvwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of France is\"\n",
        "#tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors = 'pt')\n",
        "print(input_ids)\n",
        "input_ids = input_ids['input_ids']\n",
        "print(type(input_ids))\n",
        "input_ids = input_ids.to('cuda')  # run input_ids on gpu for faster processing\n",
        "print(type(input_ids))\n",
        "\n",
        "# in transformer architecture model.model(input_ids) gives the hidden state(core transformer) before they are passed through lm_head.\n",
        "# model.lm_head(input_ids) on the other hand is responsible for converting those hidden states into predictions over the vocabulary.(final prediction layer)\n",
        "\n",
        "# Get the output of the model before the lm_head\n",
        "model_output = model.model(input_ids)\n",
        "print(type(model_output))\n",
        "\n",
        "#get the output of the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])  #selecting the first dimention in the batch\n",
        "print(lm_head_output.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ADfrnfaL-Jl",
        "outputId": "2b529fbb-77ff-482a-d1b9-a2dee87ac2a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 450, 7483,  310, 3444,  338]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
            "torch.Size([1, 5, 32064])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output\n",
        "print(token_id)\n",
        "\n",
        "token_id = lm_head_output[0, -1] #retrieves the tensor at the very last position in the sequence\n",
        "print(token_id.shape)\n",
        "print(token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Noj_Z3YLI2",
        "outputId": "5f5f6fe4-7717-4f0c-c561-3edbcad7c7a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[24.7500, 24.8750, 22.7500,  ..., 19.0000, 19.0000, 19.0000],\n",
            "         [31.1250, 31.5000, 26.0000,  ..., 26.0000, 26.0000, 26.0000],\n",
            "         [31.5000, 28.8750, 31.1250,  ..., 26.2500, 26.2500, 26.2500],\n",
            "         [33.0000, 31.8750, 36.0000,  ..., 27.8750, 27.8750, 27.8750],\n",
            "         [27.8750, 29.5000, 28.1250,  ..., 20.5000, 20.5000, 20.5000]]],\n",
            "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
            "torch.Size([32064])\n",
            "tensor([27.8750, 29.5000, 28.1250,  ..., 20.5000, 20.5000, 20.5000],\n",
            "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = token_id = lm_head_output[0, -1].argmax(-1)  #finding the index of highest probability\n",
        "print(token_id)\n",
        "output = tokenizer.decode(token_id)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OMEA6A1d15H",
        "outputId": "e8068b45-28e2-4708-8a45-e41c74a00f22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3681, device='cuda:0')\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speeding up generation by caching keys and values\n"
      ],
      "metadata": {
        "id": "moIS-hTJfofN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(\"cuda\")"
      ],
      "metadata": {
        "id": "lktobEu7fpK5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adFfLAdav5by",
        "outputId": "51502135-cd1b-4018-fd29-ad18e51f00c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.71 s ± 362 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqY9cOA2wKfn",
        "outputId": "9dd6d4b1-bbac-428e-b3e5-690ce548c538"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 s ± 753 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHY6cCgYwITv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}